{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to follow\n",
    "\n",
    "- clome the mmdetection repo: https://github.com/open-mmlab/mmdetection\n",
    "- pip install -e . in the mmdetection directory\n",
    "- Change the file: mmdetection/configs/htc/htc_r50_fpn_1x_artifact.py\n",
    "\n",
    "\n",
    "python tools/train.py configs/htc/htc_r50_fpn_1x_artifact.py --cfg-options device=mps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Train images: 69\n",
      "Validation images: 25\n",
      "Test images: 10\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Paths to your COCO annotation files\n",
    "train_anno = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json'\n",
    "valid_anno = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json'\n",
    "test_anno = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json'\n",
    "\n",
    "# Load each dataset\n",
    "coco_train = COCO(train_anno)\n",
    "coco_valid = COCO(valid_anno)\n",
    "coco_test = COCO(test_anno)\n",
    "\n",
    "# Print number of images\n",
    "print(f\"Train images: {len(coco_train.getImgIds())}\")\n",
    "print(f\"Validation images: {len(coco_valid.getImgIds())}\")\n",
    "print(f\"Test images: {len(coco_test.getImgIds())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "✅ Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "✅ Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def fix_annotation_ids(coco_json_path, save_path=None):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for idx, ann in enumerate(data['annotations']):\n",
    "        ann['id'] = idx + 1  # reassign unique IDs\n",
    "\n",
    "    if save_path is None:\n",
    "        save_path = coco_json_path  # overwrite in place\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "    print(f'✅ Fixed annotation IDs in: {save_path}')\n",
    "\n",
    "\n",
    "# Run for all sets\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 36 missing image(s) in /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train:\n",
      "  - 28426_png.rf.6d37c582b54b7376df1efc616b4ac925.jpg\n",
      "  - 29358_png.rf.c9817f81a85f83c5ac581d3a0904438f.jpg\n",
      "  - 28286_png.rf.92621a47362fc239ddbd7245c423abfc.jpg\n",
      "  - 28379_png.rf.16758c765644ce04595291e2be030848.jpg\n",
      "  - 28085_png.rf.d2450b5f47d8e0cd7edf090bb51b754d.jpg\n",
      "  - 24032_png.rf.64889bdb26661b6d5d83bb9f98c142c7.jpg\n",
      "  - 28263_png.rf.e8e7e23fd0b72520d9f91e07b95c4056.jpg\n",
      "  - 28005_png.rf.942a510ae90494f8c6f49e740f55d56b.jpg\n",
      "  - 28376_png.rf.d8fa98e06b6520a4490a694d9464a337.jpg\n",
      "  - 24040_png.rf.feec14e27a19b024b0caa52aedccf66c.jpg\n",
      "  - 28429_png.rf.dd8ba05eb1e698925ed0d03b5af821c1.jpg\n",
      "  - 28286_png.rf.d7eeab89aa703f7fec38681d8e304e09.jpg\n",
      "  - 28375_png.rf.b9a9f883f089f0d6b88b50711f607b6c.jpg\n",
      "  - 29358_png.rf.b443e8a3b54eb87c46986f84d961f8d4.jpg\n",
      "  - 28286_png.rf.af35dd986ea1f6d8f6d1566d8eee034f.jpg\n",
      "  - 28376_png.rf.aefe3879b543779e2aa4c44059a26d18.jpg\n",
      "  - 28398_png.rf.f220ba49b1e9dc886a10dae3d6174199.jpg\n",
      "  - 28453_png.rf.c8cdd84a1cd9beae98ca55f3fc5f9848.jpg\n",
      "  - 28263_png.rf.fd4daf2f4ee2e8a4b74f144208916169.jpg\n",
      "  - 28290_png.rf.7923ff43b32928df66fdccf08d292e06.jpg\n",
      "  - 28290_png.rf.f87799210567ec960b1d4863d5829fe7.jpg\n",
      "  - 28379_png.rf.dd743436f64d946e7bd501fe594b5401.jpg\n",
      "  - 24032_png.rf.5847e96025adf38404f7e536918134b8.jpg\n",
      "  - 28375_png.rf.72f6cafedac17b3fea5868ed05b183d6.jpg\n",
      "  - 28286_png.rf.f0ab406e47f93686dc9c920a9c13033b.jpg\n",
      "  - 28005_png.rf.58913b53deb4aa91ab07ac6c91c93b80.jpg\n",
      "  - 28453_png.rf.cc5eada19a5885301002b7a30c2e1222.jpg\n",
      "  - 28263_png.rf.854e32e4b6f4f92c52ae68b276a8e887.jpg\n",
      "  - 28085_png.rf.709108120eae7671bd05e88c23b79e06.jpg\n",
      "  - 28263_png.rf.2f25d3c8fea7e3cf2f34098dbedf31b8.jpg\n",
      "  - 28379_png.rf.8a2dc5ef879e02ba54dee140bf64ec49.jpg\n",
      "  - 29375_png.rf.0d54bdf117fefd2cd53505422bc3e73d.jpg\n",
      "  - 28251_png.rf.fd675ff5bc06b8913b528e624ad9be31.jpg\n",
      "  - 28005_png.rf.5b362b0ccba20786632b6e8cb50b89da.jpg\n",
      "  - 28251_png.rf.4067fa48ed834eeadd599fe8c6bc866c.jpg\n",
      "  - 28085_png.rf.efb3803a76cbf32f41e8da2100574ac6.jpg\n",
      "❌ 6 missing image(s) in /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid:\n",
      "  - 28453_png.rf.ce7e86f0fbcdb165dd08ad40fd5453f8.jpg\n",
      "  - 28290_png.rf.aff9cb2aed4ec5719b322eb1ace4f577.jpg\n",
      "  - 24040_png.rf.ba459c11f64fe4e140b08085105f3415.jpg\n",
      "  - 28383_png.rf.c551d2e4047d2c386ed53aad875ac778.jpg\n",
      "  - 28438_png.rf.d732b608c44aecc4ac4045d2d1a8565a.jpg\n",
      "  - 28383_png.rf.fe6e785481acb313bf54115f37587410.jpg\n",
      "✅ All 10 images in /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test are present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def check_image_files(annotation_path, image_folder):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    image_filenames = [img['file_name'] for img in coco_data['images']]\n",
    "    missing = []\n",
    "\n",
    "    for filename in image_filenames:\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        if not os.path.isfile(image_path):\n",
    "            missing.append(filename)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"❌ {len(missing)} missing image(s) in {image_folder}:\")\n",
    "        for m in missing:\n",
    "            print(f\"  - {m}\")\n",
    "    else:\n",
    "        print(f\"✅ All {len(image_filenames)} images in {image_folder} are present.\")\n",
    "\n",
    "\n",
    "# Run on all sets\n",
    "check_image_files(\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json',\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train'\n",
    ")\n",
    "\n",
    "check_image_files(\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json',\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid'\n",
    ")\n",
    "\n",
    "check_image_files(\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json',\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "❌ Missing 36 files from disk (referenced in JSON):\n",
      "  - 24032_png.rf.5847e96025adf38404f7e536918134b8.jpg\n",
      "  - 24032_png.rf.64889bdb26661b6d5d83bb9f98c142c7.jpg\n",
      "  - 24040_png.rf.feec14e27a19b024b0caa52aedccf66c.jpg\n",
      "  - 28005_png.rf.58913b53deb4aa91ab07ac6c91c93b80.jpg\n",
      "  - 28005_png.rf.5b362b0ccba20786632b6e8cb50b89da.jpg\n",
      "  - 28005_png.rf.942a510ae90494f8c6f49e740f55d56b.jpg\n",
      "  - 28085_png.rf.709108120eae7671bd05e88c23b79e06.jpg\n",
      "  - 28085_png.rf.d2450b5f47d8e0cd7edf090bb51b754d.jpg\n",
      "  - 28085_png.rf.efb3803a76cbf32f41e8da2100574ac6.jpg\n",
      "  - 28251_png.rf.4067fa48ed834eeadd599fe8c6bc866c.jpg\n",
      "  - 28251_png.rf.fd675ff5bc06b8913b528e624ad9be31.jpg\n",
      "  - 28263_png.rf.2f25d3c8fea7e3cf2f34098dbedf31b8.jpg\n",
      "  - 28263_png.rf.854e32e4b6f4f92c52ae68b276a8e887.jpg\n",
      "  - 28263_png.rf.e8e7e23fd0b72520d9f91e07b95c4056.jpg\n",
      "  - 28263_png.rf.fd4daf2f4ee2e8a4b74f144208916169.jpg\n",
      "  - 28286_png.rf.92621a47362fc239ddbd7245c423abfc.jpg\n",
      "  - 28286_png.rf.af35dd986ea1f6d8f6d1566d8eee034f.jpg\n",
      "  - 28286_png.rf.d7eeab89aa703f7fec38681d8e304e09.jpg\n",
      "  - 28286_png.rf.f0ab406e47f93686dc9c920a9c13033b.jpg\n",
      "  - 28290_png.rf.7923ff43b32928df66fdccf08d292e06.jpg\n",
      "  - 28290_png.rf.f87799210567ec960b1d4863d5829fe7.jpg\n",
      "  - 28375_png.rf.72f6cafedac17b3fea5868ed05b183d6.jpg\n",
      "  - 28375_png.rf.b9a9f883f089f0d6b88b50711f607b6c.jpg\n",
      "  - 28376_png.rf.aefe3879b543779e2aa4c44059a26d18.jpg\n",
      "  - 28376_png.rf.d8fa98e06b6520a4490a694d9464a337.jpg\n",
      "  - 28379_png.rf.16758c765644ce04595291e2be030848.jpg\n",
      "  - 28379_png.rf.8a2dc5ef879e02ba54dee140bf64ec49.jpg\n",
      "  - 28379_png.rf.dd743436f64d946e7bd501fe594b5401.jpg\n",
      "  - 28398_png.rf.f220ba49b1e9dc886a10dae3d6174199.jpg\n",
      "  - 28426_png.rf.6d37c582b54b7376df1efc616b4ac925.jpg\n",
      "  - 28429_png.rf.dd8ba05eb1e698925ed0d03b5af821c1.jpg\n",
      "  - 28453_png.rf.c8cdd84a1cd9beae98ca55f3fc5f9848.jpg\n",
      "  - 28453_png.rf.cc5eada19a5885301002b7a30c2e1222.jpg\n",
      "  - 29358_png.rf.b443e8a3b54eb87c46986f84d961f8d4.jpg\n",
      "  - 29358_png.rf.c9817f81a85f83c5ac581d3a0904438f.jpg\n",
      "  - 29375_png.rf.0d54bdf117fefd2cd53505422bc3e73d.jpg\n",
      "\n",
      "🟡 Extra 1 files on disk (not referenced in JSON):\n",
      "  - _annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Load train annotation\n",
    "train_anno = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json'\n",
    "img_dir = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train'\n",
    "coco = COCO(train_anno)\n",
    "\n",
    "# All file names in COCO JSON\n",
    "json_files = set(img['file_name'] for img in coco.dataset['images'])\n",
    "\n",
    "# All actual image files (case insensitive)\n",
    "actual_files = set(p.name for p in Path(img_dir).glob(\"*.*\"))\n",
    "\n",
    "# Compare\n",
    "missing = json_files - actual_files\n",
    "extra = actual_files - json_files\n",
    "\n",
    "print(f\"❌ Missing {len(missing)} files from disk (referenced in JSON):\")\n",
    "for m in sorted(missing): print(f\"  - {m}\")\n",
    "\n",
    "print(f\"\\n🟡 Extra {len(extra)} files on disk (not referenced in JSON):\")\n",
    "for e in sorted(extra): print(f\"  - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split complete. Test now has 10 images. Others redistributed to train and valid.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "base_dir = Path('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/')\n",
    "train_path = base_dir / 'train/_annotations.coco.json'\n",
    "valid_path = base_dir / 'valid/_annotations.coco.json'\n",
    "test_path  = base_dir / 'test/_annotations.coco.json'\n",
    "\n",
    "# Load annotation files\n",
    "with open(train_path) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(valid_path) as f:\n",
    "    valid_data = json.load(f)\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Shuffle and split test images\n",
    "random.seed(42)\n",
    "random.shuffle(test_data['images'])\n",
    "\n",
    "test_images = test_data['images'][:10]\n",
    "extra_images = test_data['images'][10:]\n",
    "\n",
    "# Get image ids to move\n",
    "extra_ids = {img['id'] for img in extra_images}\n",
    "test_ids = {img['id'] for img in test_images}\n",
    "\n",
    "# Separate corresponding annotations\n",
    "extra_annotations = [ann for ann in test_data['annotations'] if ann['image_id'] in extra_ids]\n",
    "test_annotations  = [ann for ann in test_data['annotations'] if ann['image_id'] in test_ids]\n",
    "\n",
    "# Split the extra images between train and val\n",
    "extra_val = extra_images[:6]\n",
    "extra_train = extra_images[6:]\n",
    "\n",
    "extra_val_ids = {img['id'] for img in extra_val}\n",
    "extra_train_ids = {img['id'] for img in extra_train}\n",
    "\n",
    "extra_val_annotations = [ann for ann in extra_annotations if ann['image_id'] in extra_val_ids]\n",
    "extra_train_annotations = [ann for ann in extra_annotations if ann['image_id'] in extra_train_ids]\n",
    "\n",
    "# Update original files\n",
    "train_data['images'].extend(extra_train)\n",
    "train_data['annotations'].extend(extra_train_annotations)\n",
    "\n",
    "valid_data['images'].extend(extra_val)\n",
    "valid_data['annotations'].extend(extra_val_annotations)\n",
    "\n",
    "test_data['images'] = test_images\n",
    "test_data['annotations'] = test_annotations\n",
    "\n",
    "# Save back to disk\n",
    "with open(train_path, 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(valid_path, 'w') as f:\n",
    "    json.dump(valid_data, f)\n",
    "\n",
    "with open(test_path, 'w') as f:\n",
    "    json.dump(test_data, f)\n",
    "\n",
    "print(\"✅ Split complete. Test now has 10 images. Others redistributed to train and valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Move extra_train images from test/ to train/\n",
    "for img in extra_train:\n",
    "    img_file = img['file_name']\n",
    "    src = base_dir / 'test' / img_file\n",
    "    dst = base_dir / 'train' / img_file\n",
    "    if src.exists():\n",
    "        shutil.move(str(src), str(dst))\n",
    "    else:\n",
    "        print(f\"❌ Missing file: {src}\")\n",
    "\n",
    "# Move extra_val images from test/ to valid/\n",
    "for img in extra_val:\n",
    "    img_file = img['file_name']\n",
    "    src = base_dir / 'test' / img_file\n",
    "    dst = base_dir / 'valid' / img_file\n",
    "    if src.exists():\n",
    "        shutil.move(str(src), str(dst))\n",
    "    else:\n",
    "        print(f\"❌ Missing file: {src}\")\n",
    "\n",
    "# Keep only test_images in test/ folder\n",
    "test_img_filenames = {img['file_name'] for img in test_images}\n",
    "for img_file in (base_dir / 'test').glob(\"*.jpg\"):\n",
    "    if img_file.name not in test_img_filenames:\n",
    "        img_file.unlink()  # Remove unneeded file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Normalized categories in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "✅ Normalized categories in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "✅ Normalized categories in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def normalize_categories(coco_path):\n",
    "    with open(coco_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Fix categories\n",
    "    data['categories'] = [\n",
    "        {'id': 0, 'name': 'artefact', 'supercategory': 'none'}\n",
    "    ]\n",
    "\n",
    "    # Fix category_ids in annotations\n",
    "    for ann in data['annotations']:\n",
    "        ann['category_id'] = 0\n",
    "\n",
    "    with open(coco_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print(f\"✅ Normalized categories in: {coco_path}\")\n",
    "\n",
    "normalize_categories('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json')\n",
    "normalize_categories('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json')\n",
    "normalize_categories('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "✅ Cleaned: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "✅ Cleaned: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_coco_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Deduplicate category\n",
    "    data['categories'] = [{'id': 0, 'name': 'artefact', 'supercategory': 'none'}]\n",
    "\n",
    "    # Fix category IDs\n",
    "    for ann in data['annotations']:\n",
    "        ann['category_id'] = 0\n",
    "\n",
    "    # Reassign image IDs\n",
    "    id_map = {}\n",
    "    for new_id, img in enumerate(data['images']):\n",
    "        old_id = img['id']\n",
    "        id_map[old_id] = new_id\n",
    "        img['id'] = new_id\n",
    "\n",
    "    for ann in data['annotations']:\n",
    "        ann['image_id'] = id_map[ann['image_id']]\n",
    "\n",
    "    # Remove images with no annotations (only if it's val/test!)\n",
    "    ann_ids = {ann['image_id'] for ann in data['annotations']}\n",
    "    data['images'] = [img for img in data['images'] if img['id'] in ann_ids]\n",
    "\n",
    "    # Save\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Cleaned: {json_path}\")\n",
    "\n",
    "# Run on all 3\n",
    "base_path = Path('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/')\n",
    "clean_coco_json(base_path / 'train/_annotations.coco.json')\n",
    "clean_coco_json(base_path / 'valid/_annotations.coco.json')\n",
    "clean_coco_json(base_path / 'test/_annotations.coco.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "✅ Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "✅ Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_annotation_ids(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Reassign unique annotation IDs\n",
    "    for new_id, ann in enumerate(data['annotations']):\n",
    "        ann['id'] = new_id + 1\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Fixed annotation IDs in: {json_path}\")\n",
    "\n",
    "# Just for train (the one throwing the error)\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified htc_r50_fpn_1x_artifact.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_base_ = './htc_r50_fpn_1x_coco.py'\n",
    "\n",
    "# 1. Modify dataset classes\n",
    "classes = ('artefact',)\n",
    "\n",
    "# 2. Update dataset paths\n",
    "data_root = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/'\n",
    "# data = dict(\n",
    "#     samples_per_gpu=2,\n",
    "#     workers_per_gpu=2,\n",
    "#     train=dict(\n",
    "#         type='CocoDataset',\n",
    "#         ann_file=data_root + 'train/_annotations.coco.json',\n",
    "#         img_prefix=data_root + 'train/',\n",
    "#         classes=classes\n",
    "#     ),\n",
    "#     val=dict(\n",
    "#         type='CocoDataset',\n",
    "#         ann_file=data_root + 'valid/_annotations.coco.json',\n",
    "#         img_prefix=data_root + 'valid/',\n",
    "#         classes=classes\n",
    "#     ),\n",
    "#     test=dict(\n",
    "#         type='CocoDataset',\n",
    "#         ann_file=data_root + 'test/_annotations.coco.json',\n",
    "#         img_prefix=data_root + 'test/',\n",
    "#         classes=classes\n",
    "#     )\n",
    "# )\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
    "    dataset=dict(\n",
    "        type='CocoDataset',\n",
    "        data_root=data_root,\n",
    "        ann_file='train/_annotations.coco.json',\n",
    "        data_prefix=dict(img='train/'),\n",
    "        metainfo=dict(classes=classes),\n",
    "        filter_cfg=dict(filter_empty_gt=False),\n",
    "        pipeline=[\n",
    "            dict(type='LoadImageFromFile'),\n",
    "            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "            dict(type='RandomFlip', prob=0.5),\n",
    "\n",
    "            # AutoAugment with scale changes — simulates variable resolution\n",
    "            dict(\n",
    "                type='AutoAugment',\n",
    "                policies=[\n",
    "                    [dict(type='Resize', scale=(1333, 640), keep_ratio=True)],\n",
    "                    [dict(type='Resize', scale=(1333, 800), keep_ratio=True)],\n",
    "                    [dict(type='Resize', scale=(1333, 960), keep_ratio=True)]\n",
    "                ]\n",
    "            ),\n",
    "\n",
    "            # Mild brightness/contrast jitter — simulates different lighting\n",
    "            dict(\n",
    "                type='PhotoMetricDistortion',\n",
    "                brightness_delta=16,\n",
    "                contrast_range=(0.9, 1.1),\n",
    "                saturation_range=(0.95, 1.05),\n",
    "                hue_delta=4\n",
    "            ),\n",
    "\n",
    "            dict(type='PackDetInputs')\n",
    "        ]\n",
    "        # pipeline=[\n",
    "        #     dict(type='LoadImageFromFile'),\n",
    "        #     dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "        #     dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
    "        #     dict(type='RandomFlip', prob=0.5),\n",
    "        #     dict(type='PhotoMetricDistortion'),\n",
    "        #     dict(type='PackDetInputs')\n",
    "        # ]\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type='CocoDataset',\n",
    "        data_root=data_root,\n",
    "        ann_file='valid/_annotations.coco.json',\n",
    "        data_prefix=dict(img='valid/'),\n",
    "        metainfo=dict(classes=classes),\n",
    "        filter_cfg=dict(filter_empty_gt=False),\n",
    "        pipeline=[\n",
    "            dict(type='LoadImageFromFile'),\n",
    "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
    "            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "            dict(\n",
    "                type='PackDetInputs',\n",
    "                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "test_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type='CocoDataset',\n",
    "        data_root=data_root,\n",
    "        ann_file='test/_annotations.coco.json',\n",
    "        data_prefix=dict(img='test/'),\n",
    "        metainfo=dict(classes=classes),\n",
    "        filter_cfg=dict(filter_empty_gt=False),  # 👈 for safety\n",
    "        pipeline=[\n",
    "            dict(type='LoadImageFromFile'),\n",
    "            dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
    "            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "            dict(\n",
    "                type='PackDetInputs',\n",
    "                meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'))\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "val_evaluator = dict(\n",
    "    type='CocoMetric',\n",
    "    ann_file=data_root + 'valid/_annotations.coco.json',\n",
    "    metric=['bbox']\n",
    ")\n",
    "\n",
    "test_evaluator = val_evaluator\n",
    "\n",
    "# 3. Adjust model to match 1 class\n",
    "model = dict(\n",
    "    backbone=dict(\n",
    "    frozen_stages=1  # maybe 2 later (for me to check after training)\n",
    "    ),\n",
    "    roi_head=dict(\n",
    "        bbox_head=[\n",
    "            dict(type='Shared2FCBBoxHead', num_classes=1),\n",
    "            dict(type='Shared2FCBBoxHead', num_classes=1),\n",
    "            dict(type='Shared2FCBBoxHead', num_classes=1)\n",
    "        ],\n",
    "        mask_head=None,\n",
    "        semantic_roi_extractor=None,\n",
    "        semantic_head=None\n",
    "    ),\n",
    "\n",
    "    rpn_head=dict(\n",
    "        type='RPNHead',\n",
    "        in_channels=256,\n",
    "        feat_channels=256,\n",
    "        anchor_generator=dict(\n",
    "            type='AnchorGenerator',\n",
    "            scales=[8],\n",
    "            ratios=[0.5, 1.0, 2.0],\n",
    "            strides=[4, 8, 16, 32, 64]),\n",
    "        bbox_coder=dict(\n",
    "            type='DeltaXYWHBBoxCoder',\n",
    "            target_means=[.0, .0, .0, .0],\n",
    "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
    "        loss_cls=dict(\n",
    "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
    "        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0),\n",
    "        test_cfg=dict(\n",
    "            nms_pre=2000,\n",
    "            max_per_img=1000,\n",
    "            nms=dict(type='nms', iou_threshold=0.7),\n",
    "            min_bbox_size=0\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    test_cfg=dict(\n",
    "        rpn=dict(\n",
    "            nms_pre=2000,\n",
    "            max_per_img=1000,\n",
    "            nms=dict(type='nms', iou_threshold=0.7),\n",
    "            min_bbox_size=0\n",
    "        ),\n",
    "        rcnn=dict(\n",
    "            score_thr=0.05,\n",
    "            nms=dict(type='nms', iou_threshold=0.5),\n",
    "            max_per_img=100\n",
    "        )\n",
    "    ),\n",
    "\n",
    "    train_cfg=dict(\n",
    "        rpn=dict(\n",
    "            assigner=dict(\n",
    "                type='MaxIoUAssigner',\n",
    "                pos_iou_thr=0.7,\n",
    "                neg_iou_thr=0.3,\n",
    "                min_pos_iou=0.3,\n",
    "                ignore_iof_thr=-1\n",
    "            ),\n",
    "            sampler=dict(\n",
    "                type='RandomSampler',\n",
    "                num=256,\n",
    "                pos_fraction=0.5,\n",
    "                neg_pos_ub=-1,\n",
    "                add_gt_as_proposals=False\n",
    "            ),\n",
    "            allowed_border=0,\n",
    "            pos_weight=-1,\n",
    "            debug=False,\n",
    "            nms=dict(type='nms', iou_threshold=0.7),\n",
    "            nms_pre=2000,\n",
    "            max_per_img=1000,\n",
    "            min_bbox_size=0\n",
    "        ),\n",
    "        rpn_proposal=dict(\n",
    "            nms_pre=2000,\n",
    "            max_per_img=1000,\n",
    "            nms=dict(type='nms', iou_threshold=0.7),\n",
    "            min_bbox_size=0\n",
    "        ),\n",
    "        rcnn=[\n",
    "            dict(\n",
    "                assigner=dict(\n",
    "                    type='MaxIoUAssigner',\n",
    "                    pos_iou_thr=0.5,\n",
    "                    neg_iou_thr=0.5,\n",
    "                    min_pos_iou=0.5,\n",
    "                    ignore_iof_thr=-1),\n",
    "                sampler=dict(\n",
    "                    type='RandomSampler',\n",
    "                    num=512,\n",
    "                    pos_fraction=0.25,\n",
    "                    neg_pos_ub=-1,\n",
    "                    add_gt_as_proposals=True),\n",
    "                mask_size=28,\n",
    "                pos_weight=-1,\n",
    "                debug=False\n",
    "            ) for _ in range(3)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "model['train_cfg'] = dict(\n",
    "    rpn=dict(\n",
    "        nms=dict(type='nms', iou_threshold=0.7)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Shorten training (for Mac)\n",
    "# runner = dict(type='EpochBasedRunner', max_epochs=6)\n",
    "\n",
    "# Early stopping but on iterations not epochs\n",
    "# train_cfg = dict(\n",
    "#     type='IterBasedTrainLoop', max_iters=10000, val_interval=150\n",
    "# )\n",
    "\n",
    "runner = dict(\n",
    "    type='IterBasedTrainLoop',\n",
    "    max_iters=10000,\n",
    "    val_interval=50 # or 150\n",
    ")\n",
    "\n",
    "\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "\n",
    "# Add early stopping hook\n",
    "custom_hooks = [\n",
    "    dict(\n",
    "        type='EarlyStoppingHook',\n",
    "        monitor='coco/bbox_mAP',  # or 'bbox_mAP_50', check your val_evaluator logs\n",
    "        rule='greater',  # because higher mAP is better\n",
    "        patience=200 #maybe 30  # stops training if no improvement in X val intervals\n",
    "    )\n",
    "]\n",
    "\n",
    "custom_hooks += [\n",
    "    dict(\n",
    "        type='DetVisualizationHook',\n",
    "        interval=150,  # only do at the very end\n",
    "        draw=True,\n",
    "        test_out_dir='work_dirs/htc_r50_artifact/vis_results'  # saves images here\n",
    "    )\n",
    "]\n",
    "\n",
    "default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type='CheckpointHook',\n",
    "        interval=1000,  # How often to save\n",
    "        save_best='coco/bbox_mAP',  # Metric to track for best\n",
    "        rule='greater',  # Maximize the metric\n",
    "        max_keep_ckpts=1  # Only keep best to save space\n",
    "    ),\n",
    "    logger=dict(type='LoggerHook', interval=50)\n",
    ")\n",
    "\n",
    "visualizer = dict(\n",
    "    type='DetLocalVisualizer',\n",
    "    vis_backends=[dict(type='LocalVisBackend')],\n",
    "    name='vis'\n",
    ")\n",
    "\n",
    "# 5. Set working directory\n",
    "work_dir = './work_dirs/htc_r50_artifact'\n",
    "load_from = 'checkpoints/htc_r50.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists('/Users/jbm/Desktop/work_dirs/htc_r50_artifact/best_coco_bbox_mAP_epoch_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for roi_head.bbox_head.0.fc_cls.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([81, 1024]).\n",
      "size mismatch for roi_head.bbox_head.0.fc_cls.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([81]).\n",
      "size mismatch for roi_head.bbox_head.1.fc_cls.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([81, 1024]).\n",
      "size mismatch for roi_head.bbox_head.1.fc_cls.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([81]).\n",
      "size mismatch for roi_head.bbox_head.2.fc_cls.weight: copying a param with shape torch.Size([2, 1024]) from checkpoint, the shape in current model is torch.Size([81, 1024]).\n",
      "size mismatch for roi_head.bbox_head.2.fc_cls.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([81]).\n",
      "missing keys in source state_dict: roi_head.mask_head.0.convs.0.conv.weight, roi_head.mask_head.0.convs.0.conv.bias, roi_head.mask_head.0.convs.1.conv.weight, roi_head.mask_head.0.convs.1.conv.bias, roi_head.mask_head.0.convs.2.conv.weight, roi_head.mask_head.0.convs.2.conv.bias, roi_head.mask_head.0.convs.3.conv.weight, roi_head.mask_head.0.convs.3.conv.bias, roi_head.mask_head.0.upsample.weight, roi_head.mask_head.0.upsample.bias, roi_head.mask_head.0.conv_logits.weight, roi_head.mask_head.0.conv_logits.bias, roi_head.mask_head.1.convs.0.conv.weight, roi_head.mask_head.1.convs.0.conv.bias, roi_head.mask_head.1.convs.1.conv.weight, roi_head.mask_head.1.convs.1.conv.bias, roi_head.mask_head.1.convs.2.conv.weight, roi_head.mask_head.1.convs.2.conv.bias, roi_head.mask_head.1.convs.3.conv.weight, roi_head.mask_head.1.convs.3.conv.bias, roi_head.mask_head.1.upsample.weight, roi_head.mask_head.1.upsample.bias, roi_head.mask_head.1.conv_logits.weight, roi_head.mask_head.1.conv_logits.bias, roi_head.mask_head.1.conv_res.conv.weight, roi_head.mask_head.1.conv_res.conv.bias, roi_head.mask_head.2.convs.0.conv.weight, roi_head.mask_head.2.convs.0.conv.bias, roi_head.mask_head.2.convs.1.conv.weight, roi_head.mask_head.2.convs.1.conv.bias, roi_head.mask_head.2.convs.2.conv.weight, roi_head.mask_head.2.convs.2.conv.bias, roi_head.mask_head.2.convs.3.conv.weight, roi_head.mask_head.2.convs.3.conv.bias, roi_head.mask_head.2.upsample.weight, roi_head.mask_head.2.upsample.bias, roi_head.mask_head.2.conv_logits.weight, roi_head.mask_head.2.conv_logits.bias, roi_head.mask_head.2.conv_res.conv.weight, roi_head.mask_head.2.conv_res.conv.bias, roi_head.semantic_head.lateral_convs.0.conv.weight, roi_head.semantic_head.lateral_convs.0.conv.bias, roi_head.semantic_head.lateral_convs.1.conv.weight, roi_head.semantic_head.lateral_convs.1.conv.bias, roi_head.semantic_head.lateral_convs.2.conv.weight, roi_head.semantic_head.lateral_convs.2.conv.bias, roi_head.semantic_head.lateral_convs.3.conv.weight, roi_head.semantic_head.lateral_convs.3.conv.bias, roi_head.semantic_head.lateral_convs.4.conv.weight, roi_head.semantic_head.lateral_convs.4.conv.bias, roi_head.semantic_head.convs.0.conv.weight, roi_head.semantic_head.convs.0.conv.bias, roi_head.semantic_head.convs.1.conv.weight, roi_head.semantic_head.convs.1.conv.bias, roi_head.semantic_head.convs.2.conv.weight, roi_head.semantic_head.convs.2.conv.bias, roi_head.semantic_head.convs.3.conv.weight, roi_head.semantic_head.convs.3.conv.bias, roi_head.semantic_head.conv_embedding.conv.weight, roi_head.semantic_head.conv_embedding.conv.bias, roi_head.semantic_head.conv_logits.weight, roi_head.semantic_head.conv_logits.bias\n",
      "\n",
      "\n",
      "✅ Total Parameters: 80,258,293 (80.26M)\n",
      "\n",
      "data_preprocessor: 0 (0.00M)\n",
      "backbone: 23,508,032 (23.51M)\n",
      "neck: 3,344,384 (3.34M)\n",
      "rpn_head: 593,935 (0.59M)\n",
      "roi_head: 52,811,942 (52.81M)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import load_state_dict\n",
    "from mmdet.registry import MODELS\n",
    "from mmdet.utils import register_all_modules\n",
    "from mmengine.logging import HistoryBuffer\n",
    "\n",
    "# 🔐 Safely allow PyTorch to load necessary pickled objects\n",
    "\n",
    "torch.serialization.add_safe_globals([\n",
    "    HistoryBuffer,\n",
    "    np.core.multiarray._reconstruct,\n",
    "    np.ndarray,\n",
    "    np.dtype,\n",
    "    np.float64().dtype.__class__,  # Float64DType\n",
    "    np.int64().dtype.__class__,    # ✅ NEW: Int64DType\n",
    "])\n",
    "\n",
    "# 🔧 Config and checkpoint paths\n",
    "config_file = 'mmdetection/configs/htc/htc_r50_fpn_1x_coco.py'\n",
    "checkpoint_file = '/Users/jbm/Desktop/work_dirs/htc_r50_artifact/best_coco_bbox_mAP_epoch_1.pth'\n",
    "\n",
    "# 🔄 Register MMDetection modules\n",
    "register_all_modules()\n",
    "\n",
    "# 📄 Load config and remove pretrained if present\n",
    "cfg = Config.fromfile(config_file)\n",
    "cfg.model.pop('pretrained', None)\n",
    "\n",
    "# 🏗️ Build model\n",
    "model = MODELS.build(cfg.model)\n",
    "model.eval()\n",
    "\n",
    "# 🧠 Load checkpoint (safely)\n",
    "\n",
    "ckpt = torch.load(\n",
    "    checkpoint_file,\n",
    "    map_location='cpu',\n",
    "    weights_only=False  # Only use if you trust the checkpoint source!\n",
    ")\n",
    "\n",
    "state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "load_state_dict(model, state_dict)\n",
    "\n",
    "# 📊 Total parameter count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n✅ Total Parameters: {total_params:,} ({total_params / 1e6:.2f}M)\\n\")\n",
    "\n",
    "# 📦 Per-module parameter count\n",
    "for name, module in model.named_children():\n",
    "    module_params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"{name}: {module_params:,} ({module_params / 1e6:.2f}M)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
