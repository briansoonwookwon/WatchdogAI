{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h\n",
    "\n",
    "- clome the mmdetection repo: https://github.com/open-mmlab/mmdetection\n",
    "- pip install -e . in the mmdetection directory\n",
    "- Change the file: mmdetection/configs/htc/htc_r50_fpn_1x_artifact.py\n",
    "\n",
    "\n",
    "python tools/train.py configs/htc/htc_r50_fpn_1x_artifact.py --cfg-options device=mps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data came from Roboflow. You can find the dataset here: https://universe.roboflow.com/subjective/deepfake-detection-kukoh\n",
    "\n",
    "Since we needed more data for training, we left the test set with only 10 images and moved the rest to the training and validation sets. Also, Roboflow uses a different ID format than the one used by MMDetection. Thus, we had to change the IDs to be able to use the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps followed to prepare the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing the data\n",
    "\n",
    "The number of images in each set printed below are actually the numbers after the redistribution of the data. For the actual numbers, please refer to the Roboflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Train images: 69\n",
      "Validation images: 25\n",
      "Test images: 10\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Paths to your COCO annotation files (data_artifacts is the folder with our data)\n",
    "train_anno = '../data_artifacts/train/_annotations.coco.json'\n",
    "valid_anno = '../data_artifacts/valid/_annotations.coco.json'\n",
    "test_anno = '../data_artifacts/test/_annotations.coco.json'\n",
    "\n",
    "# Load each dataset\n",
    "coco_train = COCO(train_anno)\n",
    "coco_valid = COCO(valid_anno)\n",
    "coco_test = COCO(test_anno)\n",
    "\n",
    "# Print number of images\n",
    "print(f\"Train images: {len(coco_train.getImgIds())}\")\n",
    "print(f\"Validation images: {len(coco_valid.getImgIds())}\")\n",
    "print(f\"Test images: {len(coco_test.getImgIds())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the annotation IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def fix_annotation_ids(coco_json_path, save_path=None):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for idx, ann in enumerate(data['annotations']):\n",
    "        ann['id'] = idx + 1  # reassign unique IDs\n",
    "\n",
    "    if save_path is None:\n",
    "        save_path = coco_json_path  # overwrite in place\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "    print(f'Fixed annotation IDs in: {save_path}')\n",
    "\n",
    "\n",
    "# Run for all sets\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if all images are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 69 images in /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train are present.\n",
      "All 25 images in /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid are present.\n",
      "All 10 images in /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test are present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def check_image_files(annotation_path, image_folder):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    image_filenames = [img['file_name'] for img in coco_data['images']]\n",
    "    missing = []\n",
    "\n",
    "    for filename in image_filenames:\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        if not os.path.isfile(image_path):\n",
    "            missing.append(filename)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"{len(missing)} missing image(s) in {image_folder}:\")\n",
    "        for m in missing:\n",
    "            print(f\"  - {m}\")\n",
    "    else:\n",
    "        print(f\"All {len(image_filenames)} images in {image_folder} are present.\")\n",
    "\n",
    "\n",
    "# Run on all sets\n",
    "check_image_files(\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json',\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train'\n",
    ")\n",
    "\n",
    "check_image_files(\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json',\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid'\n",
    ")\n",
    "\n",
    "check_image_files(\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json',\n",
    "    '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Missing 0 files from disk (referenced in JSON):\n",
      "\n",
      " Extra 37 files on disk (not referenced in JSON):\n",
      "  - 24032_png.rf.0319fb724c7d50e4629d79a71e67dc7d.jpg\n",
      "  - 24032_png.rf.3267f9a6117bbf9afc049efa4679f6b2.jpg\n",
      "  - 24040_png.rf.483f4b5ae4a734a7be385dd487140d01.jpg\n",
      "  - 24040_png.rf.a0550fe9b64d1b344f1f6de0d1da03b7.jpg\n",
      "  - 28005_png.rf.3f09b819455445ee7045b9c1040bb927.jpg\n",
      "  - 28005_png.rf.977ca9536c50565c6cabb4520b34e89f.jpg\n",
      "  - 28251_png.rf.10a4252d915242feac0228adc8246d4d.jpg\n",
      "  - 28251_png.rf.54a72de28eab545eeca37991be81d5a0.jpg\n",
      "  - 28251_png.rf.71cde4b376ef3e6463359709bc5f5aac.jpg\n",
      "  - 28263_png.rf.02305b1df36a237689765ba8692c82b1.jpg\n",
      "  - 28263_png.rf.303fad836a6b5ded0f588645d15b160c.jpg\n",
      "  - 28263_png.rf.91ad62d10de334b9a4d8bda9f5404f12.jpg\n",
      "  - 28375_png.rf.437e3cabb934858f6209bce220b92ea5.jpg\n",
      "  - 28376_png.rf.0d3a8141a17cbed8b312866575cc7ba5.jpg\n",
      "  - 28376_png.rf.3e5b6c6677b8224ec746beee0f6d7e98.jpg\n",
      "  - 28379_png.rf.42e04f1d0a5ed86bd042f5c3343707b3.jpg\n",
      "  - 28379_png.rf.9667b945c80efd2cebea28f26efc499d.jpg\n",
      "  - 28383_png.rf.7b23ca3ccbeddeb472572f0285c26164.jpg\n",
      "  - 28383_png.rf.7d3caf3e63f293b8448916ba7604be65.jpg\n",
      "  - 28383_png.rf.80b87dee749b6af53826ed1ecc04648e.jpg\n",
      "  - 28426_png.rf.8e55a37422b118a0c317e9be8b2cfc00.jpg\n",
      "  - 28429_png.rf.0ed23511e470c314da31b6e3b2cb79dd.jpg\n",
      "  - 28429_png.rf.22ef06ca330aa97cfd2a78d49fb0e667.jpg\n",
      "  - 28429_png.rf.b3f618d86151296f0fdee23c7414307d.jpg\n",
      "  - 28429_png.rf.bbac74f80c7f8c82568a4f523c4164cd.jpg\n",
      "  - 28438_png.rf.90199a71345f6b259390cb262084e447.jpg\n",
      "  - 28438_png.rf.bd8147bef554d574a1feee9bcbf038f3.jpg\n",
      "  - 28438_png.rf.bdee1f2530abc7ae99564c18f55f0277.jpg\n",
      "  - 28445_png.rf.b1d1c59c670059b222d6c2ecff2e7223.jpg\n",
      "  - 28445_png.rf.b3f27239fd91e553e18bf35cf7bc4ce0.jpg\n",
      "  - 28453_png.rf.10e02b6f7741ffebc46f54da2f6f85ef.jpg\n",
      "  - 28453_png.rf.22fde01425b5d8aa8c6c8f194c5a6d48.jpg\n",
      "  - 29358_png.rf.6edb13e13b60ec6b0171359c96a4436d.jpg\n",
      "  - 29358_png.rf.c7c3301b1c2a45c6a70057d50d872585.jpg\n",
      "  - 29375_png.rf.74837932f98a0fc35b1c6088261a17e4.jpg\n",
      "  - 29375_png.rf.b901dd870e059bb5da52afafd856902d.jpg\n",
      "  - _annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Load train annotation\n",
    "train_anno = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json'\n",
    "img_dir = '/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train'\n",
    "coco = COCO(train_anno)\n",
    "\n",
    "# All file names in COCO JSON\n",
    "json_files = set(img['file_name'] for img in coco.dataset['images'])\n",
    "\n",
    "# All actual image files (case insensitive)\n",
    "actual_files = set(p.name for p in Path(img_dir).glob(\"*.*\"))\n",
    "\n",
    "# Compare\n",
    "missing = json_files - actual_files\n",
    "extra = actual_files - json_files\n",
    "\n",
    "print(f\"Missing {len(missing)} files from disk (referenced in JSON):\")\n",
    "for m in sorted(missing): print(f\"  - {m}\")\n",
    "\n",
    "print(f\"\\n Extra {len(extra)} files on disk (not referenced in JSON):\")\n",
    "for e in sorted(extra): print(f\"  - {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split complete. Test now has 10 images. Others redistributed to train and valid.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "base_dir = Path('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/')\n",
    "train_path = base_dir / 'train/_annotations.coco.json'\n",
    "valid_path = base_dir / 'valid/_annotations.coco.json'\n",
    "test_path  = base_dir / 'test/_annotations.coco.json'\n",
    "\n",
    "# Load annotation files\n",
    "with open(train_path) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(valid_path) as f:\n",
    "    valid_data = json.load(f)\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Shuffle and split test images\n",
    "random.seed(42)\n",
    "random.shuffle(test_data['images'])\n",
    "\n",
    "test_images = test_data['images'][:10]\n",
    "extra_images = test_data['images'][10:]\n",
    "\n",
    "# Get image ids to move\n",
    "extra_ids = {img['id'] for img in extra_images}\n",
    "test_ids = {img['id'] for img in test_images}\n",
    "\n",
    "# Separate corresponding annotations\n",
    "extra_annotations = [ann for ann in test_data['annotations'] if ann['image_id'] in extra_ids]\n",
    "test_annotations  = [ann for ann in test_data['annotations'] if ann['image_id'] in test_ids]\n",
    "\n",
    "# Split the extra images between train and val\n",
    "extra_val = extra_images[:6]\n",
    "extra_train = extra_images[6:]\n",
    "\n",
    "extra_val_ids = {img['id'] for img in extra_val}\n",
    "extra_train_ids = {img['id'] for img in extra_train}\n",
    "\n",
    "extra_val_annotations = [ann for ann in extra_annotations if ann['image_id'] in extra_val_ids]\n",
    "extra_train_annotations = [ann for ann in extra_annotations if ann['image_id'] in extra_train_ids]\n",
    "\n",
    "# Update original files\n",
    "train_data['images'].extend(extra_train)\n",
    "train_data['annotations'].extend(extra_train_annotations)\n",
    "\n",
    "valid_data['images'].extend(extra_val)\n",
    "valid_data['annotations'].extend(extra_val_annotations)\n",
    "\n",
    "test_data['images'] = test_images\n",
    "test_data['annotations'] = test_annotations\n",
    "\n",
    "# Save back to disk\n",
    "with open(train_path, 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(valid_path, 'w') as f:\n",
    "    json.dump(valid_data, f)\n",
    "\n",
    "with open(test_path, 'w') as f:\n",
    "    json.dump(test_data, f)\n",
    "\n",
    "print(\"Split complete. Test now has 10 images. Others redistributed to train and valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Move extra_train images from test/ to train/\n",
    "for img in extra_train:\n",
    "    img_file = img['file_name']\n",
    "    src = base_dir / 'test' / img_file\n",
    "    dst = base_dir / 'train' / img_file\n",
    "    if src.exists():\n",
    "        shutil.move(str(src), str(dst))\n",
    "    else:\n",
    "        print(f\"Missing file: {src}\")\n",
    "\n",
    "# Move extra_val images from test/ to valid/\n",
    "for img in extra_val:\n",
    "    img_file = img['file_name']\n",
    "    src = base_dir / 'test' / img_file\n",
    "    dst = base_dir / 'valid' / img_file\n",
    "    if src.exists():\n",
    "        shutil.move(str(src), str(dst))\n",
    "    else:\n",
    "        print(f\"Missing file: {src}\")\n",
    "\n",
    "# Keep only test_images in test/ folder\n",
    "test_img_filenames = {img['file_name'] for img in test_images}\n",
    "for img_file in (base_dir / 'test').glob(\"*.jpg\"):\n",
    "    if img_file.name not in test_img_filenames:\n",
    "        img_file.unlink()  # Remove unneeded file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized categories in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "Normalized categories in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "Normalized categories in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def normalize_categories(coco_path):\n",
    "    with open(coco_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Fix categories\n",
    "    data['categories'] = [\n",
    "        {'id': 0, 'name': 'artefact', 'supercategory': 'none'}\n",
    "    ]\n",
    "\n",
    "    # Fix category_ids in annotations\n",
    "    for ann in data['annotations']:\n",
    "        ann['category_id'] = 0\n",
    "\n",
    "    with open(coco_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print(f\"Normalized categories in: {coco_path}\")\n",
    "\n",
    "normalize_categories('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json')\n",
    "normalize_categories('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json')\n",
    "normalize_categories('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "Cleaned: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "Cleaned: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_coco_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Deduplicate category\n",
    "    data['categories'] = [{'id': 0, 'name': 'artefact', 'supercategory': 'none'}]\n",
    "\n",
    "    # Fix category IDs\n",
    "    for ann in data['annotations']:\n",
    "        ann['category_id'] = 0\n",
    "\n",
    "    # Reassign image IDs\n",
    "    id_map = {}\n",
    "    for new_id, img in enumerate(data['images']):\n",
    "        old_id = img['id']\n",
    "        id_map[old_id] = new_id\n",
    "        img['id'] = new_id\n",
    "\n",
    "    for ann in data['annotations']:\n",
    "        ann['image_id'] = id_map[ann['image_id']]\n",
    "\n",
    "    # Remove images with no annotations (only if it's val/test!)\n",
    "    ann_ids = {ann['image_id'] for ann in data['annotations']}\n",
    "    data['images'] = [img for img in data['images'] if img['id'] in ann_ids]\n",
    "\n",
    "    # Save\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Cleaned: {json_path}\")\n",
    "\n",
    "# Run on all 3\n",
    "base_path = Path('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/')\n",
    "clean_coco_json(base_path / 'train/_annotations.coco.json')\n",
    "clean_coco_json(base_path / 'valid/_annotations.coco.json')\n",
    "clean_coco_json(base_path / 'test/_annotations.coco.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json\n",
      "Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json\n",
      "Fixed annotation IDs in: /Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_annotation_ids(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Reassign unique annotation IDs\n",
    "    for new_id, ann in enumerate(data['annotations']):\n",
    "        ann['id'] = new_id + 1\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    print(f\"Fixed annotation IDs in: {json_path}\")\n",
    "\n",
    "# Just for train (the one throwing the error)\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/train/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/valid/_annotations.coco.json')\n",
    "fix_annotation_ids('/Users/jbm/Documents/DSAN_6500/WatchdogAI/data_artifacts/test/_annotations.coco.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
